{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repo** https://github.com/aparna-k/datasci_course_materials/tree/master/assignment1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1** - Create a twitter app and create an access token. (OAuth1)\n",
    "http://docs.inboundnow.com/guide/create-twitter-application/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now copy four values into the file twitterstream.py. These values are your \"Consumer Key (API Key)\", your \"Consumer Secret (API Secret)\", your \"Access token\" and your \"Access token secret\". All four should now be visible on the \"Keys and Access Tokens\" page. (You may see \"Consumer Key (API Key)\" referred to as either \"Consumer key\" or \"API Key\" in some places in the code or on the web; all three are synonyms.) Open twitterstream.py and set the variables corresponding to the api key, api secret, access token, and access secret. You will see code like the below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "api_key = \"<Enter api key>\" \n",
    "api_secret = \"<Enter api secret>\" \n",
    "access_token_key = \"<Enter your access token key here>\" \n",
    "access_token_secret = \"<Enter your access token secret here>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I don't want my access token on a public Github repo, I'm creating the following four env variables and using that in the code.\n",
    "\n",
    "```bash\n",
    "export TWITTER_API_KEY=\"xxxx\"\n",
    "export TWITTER_API_SECRET=\"xxxx\"\n",
    "export TWITTER_ACCESS_TOKEN_KEY=\"xxx\"\n",
    "export TWITTER_ACCESS_TOKEN_SECRET=\"xxx\"\n",
    "```\n",
    "\n",
    "Then in the file `twitterstream.py`\n",
    "\n",
    "```python\n",
    "api_key = os.environ.get('TWITTER_API_KEY')\n",
    "api_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "access_token_key = os.environ.get('TWITTER_ACCESS_TOKEN_KEY')\n",
    "access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "python twitterstream.py > output.txt\n",
    "```\n",
    "\n",
    "This command pipes the output to a file. Stop the program with Ctrl-C, but wait at least 3 minutes for data to accumulate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3 style='color:blue'>Derive the sentiment of each tweet</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the sentiment of each tweet based on the sentiment scores of the terms in the tweet. \n",
    "\n",
    "The sentiment of a tweet is equivalent to the sum of the sentiment scores for each term in the tweet.\n",
    "\n",
    "To score a word, we use an AFINN list of words\n",
    "http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010\n",
    "\n",
    "AFINN is a list of English words rated for valence with an integer\n",
    "between minus five (negative) and plus five (positive). The words have\n",
    "been manually labeled by Finn Årup Nielsen in 2009-2011. The file\n",
    "is tab-separated. There are two versions:\n",
    "\n",
    "**AFINN-111**: Newest version with 2477 words and phrases.\n",
    "\n",
    "**AFINN-96**: 1468 unique words and phrases on 1480 lines. Note that there\n",
    "are 1480 lines, as some words are listed twice. The word list in not\n",
    "entirely in alphabetic ordering. \n",
    "\n",
    "We will be using AFINN-111 to compute sentiment.\n",
    "\n",
    "The file AFINN-111.txt contains a list of pre-computed sentiment scores. Each line in the file contains a word or phrase followed by a sentiment score. Each word or phrase that is found in a tweet but not found in AFINN-111.txt should be given a sentiment score of 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dict of words, scores from the AFINN-111**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "afinnfile = open(\"AFINN-111.txt\")\n",
    "scores = {} # initialize an empty dictionary\n",
    "for line in afinnfile:\n",
    "  term, score  = line.split(\"\\t\")  # The file is tab-delimited. \"\\t\" means \"tab character\"\n",
    "  scores[term] = int(score)  # Convert the score to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limited -1\n",
      "suicidal -2\n",
      "pardon 2\n",
      "desirable 2\n",
      "protest -2\n",
      "lurking -1\n",
      "controversial -2\n",
      "hating -3\n",
      "ridiculous -3\n",
      "hate -3\n",
      "aggression -2\n",
      "increase 1\n",
      "regretted -2\n",
      "violate -2\n",
      "granting 1\n",
      "attracted 1\n",
      "poorest -2\n",
      "scold -2\n",
      "bailout -2\n",
      "sorry -1\n"
     ]
    }
   ],
   "source": [
    "for key in scores.keys()[0:20]:\n",
    "    print key, scores[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the output.txt file using `json`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tweets_score = []\n",
    "with open('output.txt') as op:\n",
    "    for line in op:\n",
    "        tweet = json.loads(line)\n",
    "        if 'text' in tweet:\n",
    "            words = tweet['text'].split(' ')\n",
    "            score = 0\n",
    "            for word in words:\n",
    "                score += scores.get(word.lower(), 0)\n",
    "            tweets_score.append(score)\n",
    "        else:\n",
    "            tweets_score.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tweets_score` list has a score corresponding to each tweet in the same order as the tweets appear in the file.\n",
    "\n",
    "check `tweet_sentiment.py` for the program that takes in an AFINN file and an output file and computes sentiment for each line in a twitter stream file\n",
    "\n",
    "```bash\n",
    "python tweet_sentiment.py AFINN-111.txt output.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3 style='color:blue'>Derive the sentiment of new terms</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will be creating a script that computes the sentiment for the terms that **do not appear** in the file AFINN-111.txt\n",
    "\n",
    "Here's how you might think about the problem: We know we can use the sentiment-carrying words in AFINN-111.txt to deduce the overall sentiment of a tweet. Once you deduce the sentiment of a tweet, you can work backwards to deduce the sentiment of the non-sentiment carrying words that do not appear in AFINN-111.txt. For example, if the word soccer always appears in proximity with positive words like great and fun, then we can deduce that the term soccer itself carries a positive sentiment.\n",
    "\n",
    "You are provided with a skeleton file term_sentiment.py which accepts the same two arguments as tweet_sentiment.py and can be executed using the following command:\n",
    "\n",
    "```bash\n",
    "$ python term_sentiment.py AFINN-111.txt output.txt\n",
    "```\n",
    "\n",
    "Your script should print output to stdout. Each line of output should contain a term, followed by a space, followed by the sentiment. That is, each line should be in the format <term:string> <sentiment:float>\n",
    "\n",
    "For example, if you have the pair (\"foo\", 103.256) in Python, it should appear in the output as:\n",
    "\n",
    "```bash\n",
    "foo 103.256\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_terms_scores = {}\n",
    "with open('output.txt') as op:\n",
    "    for line in op:\n",
    "        tweet = json.loads(line)\n",
    "        if 'text' in tweet:\n",
    "            words = tweet['text'].split(' ')\n",
    "            pos_words = 1\n",
    "            neg_words = 1\n",
    "            new_terms = []\n",
    "            new_terms_score = 0\n",
    "            for word in words:\n",
    "                if scores.has_key(word):\n",
    "                    if scores[word] > 0:\n",
    "                        pos_words += 1\n",
    "                    else:\n",
    "                        neg_words += 1\n",
    "                else:\n",
    "                    new_terms.append(word)\n",
    "                ratio = float(pos_words) / float(neg_words)\n",
    "                if(ratio >= 1):\n",
    "                    new_terms_score = pos_words\n",
    "                else:\n",
    "                    new_terms_score = neg_words * -1\n",
    "                for new_term in new_terms:\n",
    "                    new_terms_scores[new_term] = new_terms_score\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1\n",
      "ورشة 1\n",
      "https://t.co/uacdHg78ZR 1\n",
      "これ、やってみたら自分はかなり少数派なものを選んでしまっていた。HAHAHA 1\n",
      "better! 2\n",
      "@Mhodc17 1\n",
      "casa, 1\n",
      "る 1\n",
      "pide 1\n",
      "ptm 1\n",
      "PARTE 1\n",
      "everybody 1\n",
      "يقول 1\n",
      "Buenos 1\n",
      "tug-of-war -3\n",
      "mansion 3\n",
      "3m 1\n",
      "otro 1\n",
      "@wylogp 1\n",
      "Guide 1\n"
     ]
    }
   ],
   "source": [
    "# Sample output\n",
    "for key in new_terms_scores.keys()[0:20]:\n",
    "    print key, new_terms_scores[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:green\">Explaination of the technique I've used:</h4>\n",
    "\n",
    "For each valid tweet:\n",
    "1. I count the number of positive words and the number of negative words. \n",
    "    - I initialize the `pos_words` and `neg_words` to 1 because I'll be using their ratio to determine general sentiment of the tweet and I didn't want a division by zero error\n",
    "\n",
    "2. For each tweet I get a ratio of number of positive terms to number of negative terms (num_positive/num_negative)\n",
    "\n",
    "3. If the ratio is greater than or equal to 1, I decide that the tweet is in general positive, else, I decide that the tweet is negative\n",
    "\n",
    "4. For a new term that was not found in the AFINN file, I score it as either number of positive words, if the tweet is positive, or else the word is scored `num_of_negative_words * (-1)`\n",
    "\n",
    "**This is obviously a very simplistic solution that does not consider non english words of filter out non textual tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">Compute Term Frequency</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python script frequency.py to compute the term frequency histogram of the livestream data you harvested from Problem 1.\n",
    "\n",
    "The frequency of a term can be calculated as \n",
    "\n",
    "```\n",
    "# of occurrences of the term in all tweets/# of occurrences of all terms in all tweets\n",
    "```\n",
    "\n",
    "The script will be run from the command line like this:\n",
    "\n",
    "```bash\n",
    "$ python frequency.py <tweet_file>\n",
    "```\n",
    "\n",
    "Your script should print output to stdout. Each line of output should contain a term, followed by a space, followed by the frequency of that term in the entire file. There should be one line per unique term in the entire file. Even if 25 tweets contain the word lol, the term lol should only appear once in your output (and the frequency will be at least 25!) Each line should be in the format <term:string> <frequency:float>\n",
    "\n",
    "For example, if you have the pair (bar, 0.1245) in Python it should appear in the output as:\n",
    "\n",
    "```bash\n",
    "$ bar 0.1245\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_dict = {}\n",
    "tweets = []\n",
    "with open('output.txt') as op:\n",
    "    for line in op:\n",
    "        tweet = json.loads(line)\n",
    "        tweets.append(tweet)\n",
    "        if 'text' in tweet:\n",
    "            words = tweet['text'].split()\n",
    "            for word in words:\n",
    "                term_freq_dict[word] = term_freq_dict.get(word, 0) + 1\n",
    "#                 print word, term_freq_dict[word]\n",
    "\n",
    "num_occurences_all_terms = sum(term_freq_dict.values())\n",
    "\n",
    "term_freq_hist_dict = {}\n",
    "\n",
    "for key in term_freq_dict.keys():\n",
    "    term_freq_hist_dict[key] = term_freq_dict[key] / float(num_occurences_all_terms)\n",
    "#     print key, term_freq_hist_dict[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">Which State is happiest?</h3>\n",
    "\n",
    "Write a Python script happiest_state.py that returns the name of the happiest state as a string.\n",
    "\n",
    "Your script happiest_state.py should take a file of tweets as input. It will be called from the command line like this:\n",
    "\n",
    "```bash\n",
    "$ python happiest_state.py <sentiment_file> <tweet_file>\n",
    "```\n",
    "The file AFINN-111.txt contains a list of pre-computed sentiment score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:green\">build a dict of US states abbr => expansions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_states_abr_to_expansion = {'ak': 'alaska',\n",
    "'al': 'alabama',\n",
    "'ar': 'arkansas',\n",
    "'as': 'american samoa',\n",
    "'az': 'arizona',\n",
    "'ca': 'california',\n",
    "'co': 'colorado',\n",
    "'ct': 'connecticut',\n",
    "'dc': 'district of columbia',\n",
    "'de': 'delaware',\n",
    "'fl': 'florida',\n",
    "'ga': 'georgia',\n",
    "'gu': 'guam',\n",
    "'hi': 'hawaii',\n",
    "'ia': 'iowa',\n",
    "'id': 'idaho',\n",
    "'il': 'illinois',\n",
    "'in': 'indiana',\n",
    "'ks': 'kansas',\n",
    "'ky': 'kentucky',\n",
    "'la': 'louisiana',\n",
    "'ma': 'massachusetts',\n",
    "'md': 'maryland',\n",
    "'me': 'maine',\n",
    "'mi': 'michigan',\n",
    "'mn': 'minnesota',\n",
    "'mo': 'missouri',\n",
    "'mp': 'northern mariana islands',\n",
    "'ms': 'mississippi',\n",
    "'mt': 'montana',\n",
    "'na': 'national',\n",
    "'nc': 'north carolina',\n",
    "'nd': 'north dakota',\n",
    "'ne': 'nebraska',\n",
    "'nh': 'new hampshire',\n",
    "'nj': 'new jersey',\n",
    "'nm': 'new mexico',\n",
    "'nv': 'nevada',\n",
    "'ny': 'new york',\n",
    "'oh': 'ohio',\n",
    "'ok': 'oklahoma',\n",
    "'or': 'oregon',\n",
    "'pa': 'pennsylvania',\n",
    "'pr': 'puerto rico',\n",
    "'ri': 'rhode island',\n",
    "'sc': 'south carolina',\n",
    "'sd': 'south dakota',\n",
    "'tn': 'tennessee',\n",
    "'tx': 'texas',\n",
    "'ut': 'utah',\n",
    "'va': 'virginia',\n",
    "'vi': 'virgin islands',\n",
    "'vt': 'vermont',\n",
    "'wa': 'washington',\n",
    "'wi': 'wisconsin',\n",
    "'wv': 'west virginia',\n",
    "'wy': 'wyoming'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:green\">build a dict of US states expansion => abbreviation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_expansion_to_abr = {v: k for k, v in us_states_abr_to_expansion.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:green\">Algorithm</h4>\n",
    "\n",
    "1. For each valid tweet:\n",
    "    1. Compute the sentiment score\n",
    "    2. Get the user's self declared location (because Twitter doesn't expose lat/lng of the user) from `tweet['user']['location']`. **Ref:** [User Object](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/user-object)\n",
    "    3. If user location has a value\n",
    "       For each word in the location string\n",
    "       1. If the location is found in `us_states_abr_to_expansion` dict, update the score for the state in `tweets_state_score`\n",
    "       2. Else if the location is found in the `us_states_expansion_to_abr` dict, get the corresponding abbreviation for the state and update the score for the state in `tweets_state_score`\n",
    "\n",
    "2. Get the state with the max score from `tweets_state_score` and print that out\n",
    "\n",
    "Full implementation of the code is in `happiest_state.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OH\n"
     ]
    }
   ],
   "source": [
    "tweets_state_score = {}\n",
    "with open('output.txt') as op:\n",
    "    for line in op:\n",
    "        tweet = json.loads(line)\n",
    "        if 'text' in tweet:\n",
    "            words = tweet['text'].split(' ')\n",
    "            score = 0\n",
    "            for word in words:\n",
    "                score += scores.get(word.lower(), 0)\n",
    "            \n",
    "            user_location = tweet['user']['location']\n",
    "            if user_location != None:\n",
    "                words_in_location = user_location.split()\n",
    "                for loc in words_in_location:\n",
    "                    loc = loc.lower()\n",
    "                    if us_states_abr_to_expansion.get(loc, False):\n",
    "                        tweets_state_score[loc] = tweets_state_score.get(loc, 0) + score\n",
    "                    elif us_states_expansion_to_abr.get(loc, False):\n",
    "                        abbr_state = us_states_expansion_to_abr[loc]\n",
    "                        tweets_state_score[abbr_state] = tweets_state_score.get(abbr_state, 0) + score\n",
    "\n",
    "print max(tweets_state_score, key=tweets_state_score.get).upper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
